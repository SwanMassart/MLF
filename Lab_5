# -*- coding: utf-8 -*-
"""
Created on Tue Mar 11 08:59:13 2025

@author: Swan
"""
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, InputLayer
import tensorflow as tf
import matplotlib.pyplot as plt


X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
y = np.array([[0], [1], [1], [0]], dtype=np.float32)

model = Sequential()
model.add(InputLayer(shape=(2,)))
model.add(Dense(2, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))

optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])


history = model.fit(X, y, epochs=1000, batch_size=1, verbose=0)


loss, accuracy = model.evaluate(X, y, verbose=0)





"""

#  Expérimentation

X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
y = np.array([[0], [1], [1], [0]], dtype=np.float32)


epochs_list = [200, 1000, 5000]  # Nombre d'époques
learning_rates = [0.01, 0.1, 0.5]  # Taux d'apprentissage
activation_functions = ['sigmoid', 'relu', 'tanh']  # Activation
batch_sizes = [1, 4, 6]  # Batch size
num_neurons_list = [2, 4, 8]  # Nombre de neurones dans la couche cachée
verbose_mode = 0  # (0 = silencieux, 1 = affichage détaillé)


for epochs in epochs_list:
    for lr in learning_rates:
        for activation in activation_functions:
            for batch_size in batch_sizes:
                for num_neurons in num_neurons_list:

                    # Création du modèle
                    model = Sequential([
                        Input(shape=(2,)),  
                        Dense(num_neurons, activation=activation),
                        Dense(1, activation='sigmoid')
                    ])

                    # Compilation avec l'optimiseur Adam
                    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
                    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

                    # Entraînement
                    history = model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=verbose_mode)

                    # Évaluation finale
                    loss, accuracy = model.evaluate(X, y, verbose=0)

                    # Résultats
                    print(f"\n Configuration testée :")
                    print(f"   Epochs: {epochs} |  Learning Rate: {lr} |  Activation: {activation}")
                    print(f"   Batch Size: {batch_size} |  Neurons: {num_neurons}")
                    print(f"  Final Accuracy: {accuracy * 100:.2f}%")

                    #  Affichage de la courbe de perte
                    plt.figure()
                    plt.plot(history.history['loss'])
                    plt.xlabel('Epochs')
                    plt.ylabel('Loss')
                    plt.title(f'Loss (Epochs={epochs}, LR={lr}, Act={activation}, Batch={batch_size}, Neurons={num_neurons})')
                    plt.show()
print('Accuracy: {:.2f}'.format(accuracy*100))

for id_x, data_sample in enumerate(X):
  prediction = model.predict(data_sample.reshape(1, -1))
  print(f"Data sample is {data_sample}, prediction from model {prediction}, ground_truth {y[id_x]}")
  
  
plt.figure()
plt.plot(history.history['loss'])
plt.xlabel('n epochs')
plt.ylabel('loss')
"""
